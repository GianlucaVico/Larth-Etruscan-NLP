{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "This notebook is used to train the tokenizers and to create the dataset used to train the NN models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Steps:\n",
    "- read the files\n",
    "- clean\n",
    "- make word list (Etruscan and English)\n",
    "- use additional English data (from nltk)\n",
    "- write the files\n",
    "- train the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import Data\n",
    "import re\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_re = re.compile(r\"[^a-zA-Z ]*((mr)|(ms)|(mrs)|(miss))[^a-zA-Z ]*\")\n",
    "remove_chars = re.compile(r\"[126\\[\\],<>]\")\n",
    "space_norm = re.compile(r\" +\")\n",
    "add_unk = re.compile(r\"\\?\")\n",
    "non_word = re.compile(r\"[^a-zA-Z ]\")\n",
    "\n",
    "def clean_english(x: str) -> str:\n",
    "    x = x.lower()\n",
    "    x = title_re.sub(\" \", x)\n",
    "    x = remove_chars.sub(\" \", x)\n",
    "    # x = add_unk.sub(\"<unk>\", x)\n",
    "    x = add_unk.sub(\" \", x) # Remove ? from training data -> ? to <unk>\n",
    "    x = space_norm.sub(\" \", x)\n",
    "    return x.strip()\n",
    "\n",
    "def clean_etruscan(x: str) -> str:\n",
    "    x = x.lower()\n",
    "    x = remove_chars.sub(\" \", x)\n",
    "    x = space_norm.sub(\" \", x)\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "et, eng = Data.load_translation_dataset(etruscan_fn=clean_etruscan, english_fn=clean_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -abcdefghiklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "et_chars = list(set(\"\".join(et)))\n",
    "et_chars = sorted(et_chars)\n",
    "print(\"\".join(et_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "eng_chars = list(set(\"\".join(eng)))\n",
    "eng_chars = sorted(eng_chars)\n",
    "print(\"\".join(eng_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etruscan_tokenizer = Data.SentencePieceTokenizer(Data.ETRUSCAN)\n",
    "english_tokenizer = Data.SentencePieceTokenizer(Data.ENGLISH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Data._dir + \"etruscan_tokenizer_data.txt\", \"wt\") as f:\n",
    "    for i in et:\n",
    "        f.write(i)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Integrate with Brown or https://github.com/cltk/grc_text_perseus/tree/master/cltk_json\n",
    "with open(Data._dir + \"english_tokenizer_data.txt\", \"wt\") as f:\n",
    "    for i in eng:\n",
    "        f.write(i)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/etruscan_word\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1927\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2891 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=36898\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9892% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=24\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999892\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2891 sentences.\n",
      "unigram_model_trainer.cc(147) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(151) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(202) LOG(INFO) Initialized 5234 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 2891\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 2348\n",
      "unigram_model_trainer.cc(492) LOG(INFO) Using 2348 sentences for EM training\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=2733 obj=58.7454 num_tokens=11957 num_tokens/piece=4.37505\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=2306 obj=57.6851 num_tokens=12633 num_tokens/piece=5.47832\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=1984 obj=57.7064 num_tokens=12943 num_tokens/piece=6.52369\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=1922 obj=57.5611 num_tokens=13178 num_tokens/piece=6.8564\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/etruscan_word.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/etruscan_word.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/etruscan_char\n",
      "  model_type: CHAR\n",
      "  vocab_size: 1927\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2891 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=36898\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9892% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=24\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999892\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2891 sentences.\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/etruscan_char.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/etruscan_char.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/english_word\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2891 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=75011\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9613% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=25\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999613\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2891 sentences.\n",
      "unigram_model_trainer.cc(147) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(151) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(202) LOG(INFO) Initialized 11236 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 2891\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 2163\n",
      "unigram_model_trainer.cc(492) LOG(INFO) Using 2163 sentences for EM training\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=3911 obj=108.738 num_tokens=20724 num_tokens/piece=5.2989\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=3219 obj=105.653 num_tokens=21404 num_tokens/piece=6.64927\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=2410 obj=105.601 num_tokens=22235 num_tokens/piece=9.22614\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=2398 obj=104.389 num_tokens=22312 num_tokens/piece=9.30442\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=2197 obj=104.474 num_tokens=22595 num_tokens/piece=10.2845\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=2194 obj=105.359 num_tokens=22602 num_tokens/piece=10.3017\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/english_word.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/english_word.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/english_char\n",
      "  model_type: CHAR\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2891 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=75011\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9613% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=25\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999613\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2891 sentences.\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/english_char.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/english_char.vocab\n"
     ]
    }
   ],
   "source": [
    "etruscan_tokenizer.train(Data._dir + \"etruscan_tokenizer_data.txt\", Data._dir + \"etruscan\", vocab_size=1927)\n",
    "english_tokenizer.train(Data._dir + \"english_tokenizer_data.txt\", Data._dir + \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Data.tokenizers.SentencePieceTokenizer at 0x7f78d93cbed0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokenizer.load(Data._dir + \"english\")\n",
    "etruscan_tokenizer.load(Data._dir + \"etruscan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 8, 5, 7, 8, 2],\n",
       " [1, 4, 13, 8, 11, 5, 12, 4, 8, 5, 7, 8, 2],\n",
       " [1, 4, 6, 10, 13, 8, 11, 5, 12, 4, 13, 10, 5, 2],\n",
       " [1, 4, 3, 2],\n",
       " [1, 2]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_chars, tmp_words = english_tokenizer.tokenize([\"test\", \"other test\", \"another one\", \"?\", \"\\n\"], align=False)\n",
    "tmp_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'other test', 'another one', '<unk>', '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokenizer.detokenize(tmp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_chars, align_words = english_tokenizer.tokenize([\"test\", \"other test\", \"another one\", \"?\", \"\\n\"], align=True)\n",
    "_, no_align_words = english_tokenizer.tokenize([\"test\", \"other test\", \"another one\", \"?\", \"\\n\"], align=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 247, 247, 247, 12, 2], [1, 4, 45, 7, 7, 7, 24, 4, 247, 247, 247, 12, 2], [1, 4, 1197, 1197, 1197, 7, 7, 7, 24, 4, 13, 13, 5, 2], [1, 4, 3, 2], [1, 2]]\n",
      "[[1, 4, 247, 12, 2], [1, 4, 45, 7, 24, 4, 247, 12, 2], [1, 4, 1197, 7, 24, 4, 13, 5, 2], [1, 4, 3, 2], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(align_words)\n",
    "print(no_align_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['testestest', 'othethether testestest', 'anoanoanothethether onone', '<unk>', '']\n",
      "['test', 'other test', 'another one', '<unk>', '']\n",
      "['test', 'other test', 'another one', '<unk>', '']\n"
     ]
    }
   ],
   "source": [
    "print(english_tokenizer.detokenize(align_words))\n",
    "print(english_tokenizer.detokenize(no_align_words))\n",
    "print(english_tokenizer.detokenize(align_chars, word=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 34, 2],\n",
       " [1, 4, 71, 4, 522, 2],\n",
       " [1, 4, 505, 23, 112, 4, 3, 4, 39, 4, 207, 5, 104, 5, 355, 2]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_chars, tmp_words = etruscan_tokenizer.tokenize([\"larth\", \"mini mlakas\", \"questo è in italiano\"], align=False)\n",
    "tmp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['larth', 'mini mlakas', 'questo <unk> in italiano']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etruscan_tokenizer.detokenize(tmp_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input tokenizer with multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lang_tokenizer = Data.SentencePieceTokenizer(Data.ENGLISH)\n",
    "extended_english_tokenizer = Data.SentencePieceTokenizer(Data.ENGLISH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Data._dir + \"latin_tokenizer_data.txt\", \"rt\") as f:\n",
    "    lat = f.readlines()\n",
    "lat = [i.strip().lower() for i in lat]\n",
    "with open(Data._dir + \"greek_tokenizer_data.txt\", \"rt\") as f:\n",
    "    grk = f.readlines()\n",
    "grk = [i.strip().lower() for i in grk]\n",
    "\n",
    "with open(Data._dir + \"english_l_tokenizer_data.txt\", \"rt\") as f:\n",
    "    eng_l = f.readlines()\n",
    "eng_l = [i.strip().lower() for i in eng_l]\n",
    "with open(Data._dir + \"english_g_tokenizer_data.txt\", \"rt\") as f:\n",
    "    eng_g = f.readlines()\n",
    "eng_g = [i.strip().lower() for i in eng_g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/multi_word\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (25116 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 36349 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 41 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=9228935\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9984% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=25\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999984\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 36349 sentences.\n",
      "unigram_model_trainer.cc(147) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(151) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(202) LOG(INFO) Initialized 290210 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 36349\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 35751\n",
      "unigram_model_trainer.cc(492) LOG(INFO) Using 35751 sentences for EM training\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=108834 obj=1514.73 num_tokens=3181666 num_tokens/piece=29.2341\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=91310 obj=1485.79 num_tokens=3248100 num_tokens/piece=35.5722\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=68463 obj=1485.86 num_tokens=3300557 num_tokens/piece=48.2094\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=68345 obj=1484.35 num_tokens=3305534 num_tokens/piece=48.3654\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=51257 obj=1465.08 num_tokens=3362354 num_tokens/piece=65.5979\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=51256 obj=1474.2 num_tokens=3376612 num_tokens/piece=65.8774\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=38442 obj=1441.19 num_tokens=3428977 num_tokens/piece=89.1987\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=38442 obj=1440.44 num_tokens=3431372 num_tokens/piece=89.261\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=28831 obj=1414.58 num_tokens=3496126 num_tokens/piece=121.263\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=28831 obj=1413.15 num_tokens=3496213 num_tokens/piece=121.266\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=21623 obj=1412.19 num_tokens=3570294 num_tokens/piece=165.116\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=21623 obj=1410.4 num_tokens=3571017 num_tokens/piece=165.149\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=16217 obj=1399.85 num_tokens=3653498 num_tokens/piece=225.288\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=16217 obj=1398.05 num_tokens=3656176 num_tokens/piece=225.453\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=12162 obj=1394.92 num_tokens=3749436 num_tokens/piece=308.291\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=12162 obj=1392.96 num_tokens=3750663 num_tokens/piece=308.392\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=9121 obj=1385.95 num_tokens=3850406 num_tokens/piece=422.147\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=9121 obj=1383.87 num_tokens=3850468 num_tokens/piece=422.154\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=6840 obj=1377.45 num_tokens=3957857 num_tokens/piece=578.634\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=6840 obj=1375.67 num_tokens=3957679 num_tokens/piece=578.608\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=5130 obj=1366.15 num_tokens=4071075 num_tokens/piece=793.582\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=5130 obj=1363.87 num_tokens=4070704 num_tokens/piece=793.51\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=4400 obj=1362.53 num_tokens=4136732 num_tokens/piece=940.166\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=4400 obj=1361.51 num_tokens=4138081 num_tokens/piece=940.473\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/multi_word.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/multi_word.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/multi_char\n",
      "  model_type: CHAR\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: -\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (25116 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 36349 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 41 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=9228935\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9984% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=25\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999984\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 36349 sentences.\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/multi_char.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/multi_char.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/extended_english_word\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4451 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 36222 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 175 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=13491755\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 36222 sentences.\n",
      "unigram_model_trainer.cc(147) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(151) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(202) LOG(INFO) Initialized 87784 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 36222\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 35473\n",
      "unigram_model_trainer.cc(492) LOG(INFO) Using 35473 sentences for EM training\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=37040 obj=2455.19 num_tokens=5173764 num_tokens/piece=139.68\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=28714 obj=2438 num_tokens=5203828 num_tokens/piece=181.23\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=21526 obj=2431.23 num_tokens=5227697 num_tokens/piece=242.855\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=21508 obj=2427.37 num_tokens=5229118 num_tokens/piece=243.124\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=16131 obj=2342.4 num_tokens=5255228 num_tokens/piece=325.784\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=16131 obj=2341.67 num_tokens=5276615 num_tokens/piece=327.11\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=12098 obj=2289.61 num_tokens=5317393 num_tokens/piece=439.527\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=12098 obj=2300.36 num_tokens=5318821 num_tokens/piece=439.645\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=9073 obj=2249.81 num_tokens=5381573 num_tokens/piece=593.142\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=9073 obj=2247.58 num_tokens=5382805 num_tokens/piece=593.277\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=6804 obj=2234.68 num_tokens=5468758 num_tokens/piece=803.756\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=6804 obj=2233.34 num_tokens=5469820 num_tokens/piece=803.912\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=5103 obj=2158.4 num_tokens=5582174 num_tokens/piece=1093.9\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=5103 obj=2156.37 num_tokens=5583521 num_tokens/piece=1094.16\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=3827 obj=2137.86 num_tokens=5733386 num_tokens/piece=1498.14\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=3827 obj=2135.31 num_tokens=5733588 num_tokens/piece=1498.19\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=2870 obj=2140.83 num_tokens=5908146 num_tokens/piece=2058.59\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=2870 obj=2137.9 num_tokens=5910590 num_tokens/piece=2059.44\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=2200 obj=2103.05 num_tokens=6083635 num_tokens/piece=2765.29\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=2200 obj=2100.9 num_tokens=6090341 num_tokens/piece=2768.34\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/extended_english_word.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/extended_english_word.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/extended_english_char\n",
      "  model_type: CHAR\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: -\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4451 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 36222 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 175 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=13491755\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 36222 sentences.\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/extended_english_char.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/extended_english_char.vocab\n"
     ]
    }
   ],
   "source": [
    "multi_lang_tokenizer.train(et + lat + grk, Data._dir + \"multi\", vocab_size = 4000)\n",
    "extended_english_tokenizer.train(eng + eng_l + eng_g, Data._dir + \"extended_english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One tokenizer for everything\n",
    "\n",
    "Use all the previous data and the Tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2021-08-07: the train split does not contain Greek text\n",
    "(t_train_grc_target,\n",
    "    t_train_grc_source,\n",
    "    t_test_grc_target, \n",
    "    t_test_grc_source, \n",
    "    t_dev_grc_target, \n",
    "    t_dev_grc_source\n",
    ") = Data.load_tatoeba(Data._dir + \"Tatoeba/data/release/v2021-08-07/eng-grc\")\n",
    "\n",
    "(t_train_lat_target,\n",
    "    t_train_lat_source,\n",
    "    t_test_lat_target, \n",
    "    t_test_lat_source, \n",
    "    t_dev_lat_target, \n",
    "    t_dev_lat_source\n",
    ") = Data.load_tatoeba(Data._dir + \"Tatoeba/data/release/v2021-08-07/eng-lat\")\n",
    "\n",
    "tatoeba_greek = t_test_grc_source + t_dev_grc_source\n",
    "tatoeba_latin = t_train_lat_source + t_test_lat_source + t_dev_lat_source\n",
    "tatoeba_english = t_train_lat_target + t_test_lat_target + t_dev_lat_target + t_test_grc_target + t_dev_grc_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tatoeba(x: str) -> str:\n",
    "    x = x.lower()\n",
    "    \n",
    "    # Split accents, remove and recombine\n",
    "    x = unicodedata.normalize('NFD', x)\n",
    "    x = ''.join(c for c in x if not unicodedata.combining(c))\n",
    "    x = unicodedata.normalize('NFC', x)\n",
    "    \n",
    "    x = x.translate(utils.greek_to_latin)\n",
    "    x = x.translate(utils.others)\n",
    "    \n",
    "    x = non_word.sub(\" \", x) # This also remove punctuation\n",
    "    x = space_norm.sub(\" \", x)\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tatoeba_greek_clean = [clean_tatoeba(i) for i in tatoeba_greek]\n",
    "tatoeba_latin_clean = [clean_tatoeba(i) for i in tatoeba_latin]\n",
    "tatoeba_english_clean = [clean_tatoeba(i) for i in tatoeba_english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " abcdefghijklmnoprstuvxyz\n",
      " abcdefghijklmnopqrstuvwxyz\n",
      " abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "tatoeba_greek_chars = sorted(list(set(\"\".join(tatoeba_greek_clean))))\n",
    "print(\"\".join(tatoeba_greek_chars))\n",
    "\n",
    "tatoeba_latin_chars = sorted(list(set(\"\".join(tatoeba_latin_clean))))\n",
    "print(\"\".join(tatoeba_latin_chars))\n",
    "\n",
    "tatoeba_english_chars = sorted(list(set(\"\".join(tatoeba_english_clean))))\n",
    "print(\"\".join(tatoeba_english_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_greek = tatoeba_greek_clean + grk\n",
    "all_latin = tatoeba_latin_clean + lat\n",
    "all_english = tatoeba_english_clean + eng_g + eng_l + eng\n",
    "all_lang = et + all_greek + all_latin + all_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "all_chars = sorted(list(set(\"\".join(all_lang))))\n",
    "print(\"\".join(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486060\n",
      "99700\n",
      "226776\n",
      "201028\n",
      "3051\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(set(\" \".join(all_lang).split()))\n",
    "print(vocab_size)\n",
    "\n",
    "vocab_size = len(set(\" \".join(all_greek).split()))\n",
    "print(vocab_size)\n",
    "\n",
    "vocab_size = len(set(\" \".join(all_latin).split()))\n",
    "print(vocab_size)\n",
    "\n",
    "vocab_size = len(set(\" \".join(all_english).split()))\n",
    "print(vocab_size)\n",
    "\n",
    "vocab_size = len(set(\" \".join(et).split()))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_english_word\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 102792\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4896 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(121) LOG(WARNING) Too many sentences are loaded! (1164934), which may slow down training.\n",
      "trainer_interface.cc(123) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(126) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1164934 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 196 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=59345477\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9924% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999924\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1164934 sentences.\n",
      "unigram_model_trainer.cc(147) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(151) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(202) LOG(INFO) Initialized 335580 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1164934\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 1042737\n",
      "unigram_model_trainer.cc(492) LOG(INFO) Using 1042737 sentences for EM training\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=123549 obj=355.913 num_tokens=20643270 num_tokens/piece=167.086\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=102787 obj=352.267 num_tokens=20763785 num_tokens/piece=202.008\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_english_word.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_english_word.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_english_char\n",
      "  model_type: CHAR\n",
      "  vocab_size: 33\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: -\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4896 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(121) LOG(WARNING) Too many sentences are loaded! (1164934), which may slow down training.\n",
      "trainer_interface.cc(123) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(126) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1164934 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 196 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=59345477\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9924% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999924\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1164934 sentences.\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_english_char.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_english_char.vocab\n"
     ]
    }
   ],
   "source": [
    "big_english_tokenizer = Data.SentencePieceTokenizer(Data.ETRUSCAN)\n",
    "big_english_tokenizer.train(all_english, name = Data._dir + \"all_english\", vocab_size=102792) # vocab size: set based on the error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_word\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 262104\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4262 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(121) LOG(WARNING) Too many sentences are loaded! (2328342), which may slow down training.\n",
      "trainer_interface.cc(123) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(126) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2328342 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 252 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=112695477\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9935% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999935\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2328342 sentences.\n",
      "unigram_model_trainer.cc(147) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(151) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(202) LOG(INFO) Initialized 843012 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 2328342\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 1559052\n",
      "unigram_model_trainer.cc(492) LOG(INFO) Using 1559052 sentences for EM training\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=313714 obj=331.068 num_tokens=30344189 num_tokens/piece=96.7256\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=262099 obj=325.275 num_tokens=30747747 num_tokens/piece=117.313\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_word.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_word.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_char\n",
      "  model_type: CHAR\n",
      "  vocab_size: 33\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: -\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4262 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(121) LOG(WARNING) Too many sentences are loaded! (2328342), which may slow down training.\n",
      "trainer_interface.cc(123) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(126) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2328342 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 252 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=112695477\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9935% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999935\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2328342 sentences.\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_char.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_char.vocab\n"
     ]
    }
   ],
   "source": [
    "big_tokenizer = Data.SentencePieceTokenizer(Data.ETRUSCAN)\n",
    "big_tokenizer.train(all_lang, name = Data._dir + \"all\", vocab_size=262104) # vocab size: set based on the error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Data.tokenizers.SentencePieceTokenizer at 0x7fc7f5b25510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_tokenizer = Data.SentencePieceTokenizer(Data.ETRUSCAN)\n",
    "big_tokenizer.load(Data._dir + \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 8, 23, 18, 16, 5, 16, 20, 21, 15, 6, 29, 27, 17, 14, 11, 10, 19, 26, 12, 9, 7, 13, 24, 25, 28, 22, 30, 3, 2]]\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(big_tokenizer.tokenize([\"abcdedfghijklmnopqrstuvwxyz-§\"], align=False)[0])\n",
    "print(big_tokenizer._sp_chars.unk_id())\n",
    "print(big_tokenizer.tokenize([\"abcdedfghijklmnopqrstuvwxyz-§\"], align=False)[0][0].count(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "print(big_tokenizer.tokenize([\"---§§§\"], align=False)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is some english text', 'larth', 'ellas', 'cane canem']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_tokenizer.detokenize(big_tokenizer.tokenize([\"this is some english text\", \"larth\", \"ellas\", \"cane canem\"], align=False)[1], word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testo in italiano']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_tokenizer.detokenize(big_tokenizer.tokenize([\"testo in italiano\"], align=False)[1], word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_small_word\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: <unk>\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4262 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(121) LOG(WARNING) Too many sentences are loaded! (2328342), which may slow down training.\n",
      "trainer_interface.cc(123) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(126) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2328342 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 252 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=112695477\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9935% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999935\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2328342 sentences.\n",
      "unigram_model_trainer.cc(147) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(151) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(202) LOG(INFO) Initialized 843012 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 2328342\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 1559052\n",
      "unigram_model_trainer.cc(492) LOG(INFO) Using 1559052 sentences for EM training\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=313714 obj=331.068 num_tokens=30344189 num_tokens/piece=96.7256\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=262099 obj=325.275 num_tokens=30747747 num_tokens/piece=117.313\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=196557 obj=325.204 num_tokens=30910985 num_tokens/piece=157.262\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=196489 obj=325.176 num_tokens=30966372 num_tokens/piece=157.599\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=147366 obj=321.455 num_tokens=31149508 num_tokens/piece=211.375\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=147365 obj=323.061 num_tokens=31170494 num_tokens/piece=211.519\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=110523 obj=308.908 num_tokens=31377485 num_tokens/piece=283.9\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=110523 obj=308.969 num_tokens=31382490 num_tokens/piece=283.945\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=82892 obj=306.4 num_tokens=31699944 num_tokens/piece=382.425\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=82892 obj=306.271 num_tokens=31717449 num_tokens/piece=382.636\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=62169 obj=307.007 num_tokens=32031104 num_tokens/piece=515.226\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=62169 obj=306.878 num_tokens=32035219 num_tokens/piece=515.292\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=46626 obj=300.744 num_tokens=32406512 num_tokens/piece=695.031\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=46626 obj=300.595 num_tokens=32409007 num_tokens/piece=695.084\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=34969 obj=294.172 num_tokens=32839710 num_tokens/piece=939.109\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=34969 obj=293.993 num_tokens=32841854 num_tokens/piece=939.171\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=26226 obj=292.128 num_tokens=33345798 num_tokens/piece=1271.48\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=26226 obj=291.084 num_tokens=33452716 num_tokens/piece=1275.56\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=19669 obj=289.823 num_tokens=34043306 num_tokens/piece=1730.81\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=19669 obj=289.572 num_tokens=34057477 num_tokens/piece=1731.53\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=14751 obj=286.888 num_tokens=34728341 num_tokens/piece=2354.3\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=14751 obj=286.614 num_tokens=34738384 num_tokens/piece=2354.99\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=11063 obj=282.96 num_tokens=35686280 num_tokens/piece=3225.73\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=11063 obj=282.596 num_tokens=35704509 num_tokens/piece=3227.38\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=0 size=11000 obj=282.716 num_tokens=35750167 num_tokens/piece=3250.02\n",
      "unigram_model_trainer.cc(508) LOG(INFO) EM sub_iter=1 size=11000 obj=282.61 num_tokens=35765152 num_tokens/piece=3251.38\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_small_word.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_small_word.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_small_char\n",
      "  model_type: CHAR\n",
      "  vocab_size: 33\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ▁\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface: -\n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4262 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(144) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(121) LOG(WARNING) Too many sentences are loaded! (2328342), which may slow down training.\n",
      "trainer_interface.cc(123) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(126) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2328342 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 252 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: ▁\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=112695477\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9935% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=26\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999935\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2328342 sentences.\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_small_char.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: /media/gianluca/Shared/DKE/MasterThesis/Translation/IthacaLike/../../Data/all_small_char.vocab\n"
     ]
    }
   ],
   "source": [
    "small_tokenizer = Data.SentencePieceTokenizer(Data.ETRUSCAN)\n",
    "small_tokenizer.train(all_lang, name = Data._dir + \"all_small\", vocab_size=10000) # vocab size: set based on the error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 8, 23, 18, 16, 5, 16, 20, 21, 15, 6, 29, 27, 17, 14, 11, 10, 19, 26, 12, 9, 7, 13, 24, 25, 28, 22, 30, 3, 2]]\n",
      "3\n",
      "1\n",
      "['this is some english text', 'larth', 'ellas', 'cane canem']\n"
     ]
    }
   ],
   "source": [
    "small_tokenizer = Data.SentencePieceTokenizer(Data.ETRUSCAN)\n",
    "small_tokenizer.load(Data._dir + \"all_small\")\n",
    "print(small_tokenizer.tokenize([\"abcdedfghijklmnopqrstuvwxyz-§\"], align=False)[0])\n",
    "print(small_tokenizer._sp_chars.unk_id())\n",
    "print(small_tokenizer.tokenize([\"abcdedfghijklmnopqrstuvwxyz-§\"], align=False)[0][0].count(3))\n",
    "print(small_tokenizer.detokenize(small_tokenizer.tokenize([\"this is some english text\", \"larth\", \"ellas\", \"cane canem\"], align=False)[1], word=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Steps:\n",
    "- read the data\n",
    "- clean\n",
    "- tokenize\n",
    "- ~~save the data (pickle? / tf)~~\n",
    "- move the functions/classes/etc... to `train.py`\n",
    "\n",
    "Dataset structure:\n",
    "- train\n",
    "    - inputs\n",
    "    - inputs_char\n",
    "    - inputs_word\n",
    "    - targets\n",
    "    - targets_char\n",
    "    - targets_word\n",
    "- test\n",
    "    - inputs\n",
    "    - inputs_char\n",
    "    - inputs_word\n",
    "    - targets\n",
    "    - targets_char\n",
    "    - targets_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import datasets\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional, Union, Dict, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(Data._dir + \"Etruscan.csv\", index_col=0).dropna(subset=[\"Translation\"]).reset_index()\n",
    "is_etp = tmp[\"key\"].isna().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_encoded_char, et_encoded_word = etruscan_tokenizer.tokenize(et)\n",
    "# et_encoded_char = etruscan_tokenizer.pad_sequences(et_encoded_char)\n",
    "# et_encoded_word = etruscan_tokenizer.pad_sequences(et_encoded_word)\n",
    "\n",
    "eng_encoded_char, eng_encoded_word = english_tokenizer.tokenize(eng)\n",
    "# eng_encoded_char = english_tokenizer.pad_sequences(eng_encoded_char)\n",
    "# eng_encoded_word = english_tokenizer.pad_sequences(eng_encoded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.Dataset.from_dict(\n",
    "    {     \n",
    "    \"inputs\": et,\n",
    "    \"inputs_chars\": et_encoded_char,\n",
    "    \"inputs_words\": et_encoded_word,\n",
    "    \"targets\": eng,\n",
    "    \"targets_chars\": eng_encoded_char,\n",
    "    \"targets_words\": eng_encoded_word,\n",
    "    \"is_etp\": is_etp\n",
    "    },\n",
    ")\n",
    "# ds = ds.with_format(\"jax\", device=str(jax.devices()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = ds.train_test_split(train_size=0.9, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Create the batched and iterate through the datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, ds: datasets.Dataset, batch_size: int, cached: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ds: dataset\n",
    "            batch_size: size of the batches\n",
    "            cached: whether to immediatly create and store the batched\n",
    "        \"\"\"\n",
    "        self.ds: datasets.Dataset = ds\n",
    "        self.batch_size: int = batch_size\n",
    "        self.cached: bool = cached\n",
    "        # init cache\n",
    "        iter(self)\n",
    "\n",
    "    def __next__(self) -> Dict[str, Union[List[str], jax.Array]]:\n",
    "        d = next(self.iterator)\n",
    "        if self.cached:\n",
    "            return d\n",
    "        return self.make_batch(d)\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        self.iterator: Iterator = self.ds.iter(self.batch_size)\n",
    "        if self.cached:\n",
    "            self.iterator = [self.make_batch(i) for i in self.iterator]\n",
    "            self.iterator = iter(self.iterator)\n",
    "        return self\n",
    "\n",
    "    def make_batch(\n",
    "        self, d: Dict[str, Union[List[str], List[List[int]]]]\n",
    "    ) -> Dict[str, Union[List[str], jax.Array]]:\n",
    "        \"\"\"\n",
    "        Create a batch with jax Arrays.\n",
    "\n",
    "        Args:\n",
    "            d: batch as a dictionary\n",
    "\n",
    "        Returns:\n",
    "            Dict: batch as jax Arrays\n",
    "        \"\"\"\n",
    "        inputs_chars, inputs_words = self.pad(d[\"inputs_chars\"], d[\"inputs_words\"])\n",
    "        targets_chars, targets_words = self.pad(d[\"targets_chars\"], d[\"targets_words\"])\n",
    "        return {\n",
    "            # \"inputs\": d[\"inputs\"],\n",
    "            \"inputs_chars\": inputs_chars,\n",
    "            \"inputs_words\": inputs_words,\n",
    "            # \"targets\": d[\"targets\"],\n",
    "            \"targets_chars\": targets_chars,\n",
    "            \"targets_words\": targets_words,\n",
    "            # \"is_etp\": d[\"is_etp\"]\n",
    "        }\n",
    "\n",
    "    def pad(self, l: List[List[int]], o: Optional[List[List[int]]]=None) -> Union[jax.Array, Tuple[jax.Array, jax.Array]]:\n",
    "        \"\"\"\n",
    "        Pad sequences and cast then to a jax Array.\n",
    "        Pad with 0\n",
    "\n",
    "        Args:\n",
    "            l: list of sequences\n",
    "            o: other sequence if needed\n",
    "        Returns:\n",
    "            jax Array\n",
    "        \"\"\"\n",
    "        l_lens = [len(i) for i in l]\n",
    "        max_l = max(l_lens)\n",
    "        if o is not None:\n",
    "            o_lens = [len(i) for i in o]\n",
    "            max_l = max(max_l, max(o_lens))\n",
    "\n",
    "        new_l = jnp.array([np.pad(i, (0, max_l - j)) for i, j in zip(l, l_lens)])\n",
    "        if o is not None:\n",
    "            new_o = jnp.array([np.pad(i, (0, max_l - j)) for i, j in zip(l, l_lens)])\n",
    "            return new_l, new_o\n",
    "        return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(etruscan_csv: str, etruscan_model: str, english_model: str, batch_size: int, train_size: float=0.9, cached: bool=False):\n",
    "    # Load & clean\n",
    "    title_re = re.compile(r\"[^a-zA-Z ]*((mr)|(ms)|(mrs)|(miss))[^a-zA-Z ]*\")\n",
    "    remove_chars = re.compile(r\"[126\\[\\],<>]\")\n",
    "    space_norm = re.compile(r\" +\")\n",
    "    add_unk = re.compile(r\"\\?\")\n",
    "\n",
    "    def clean_english(x: str) -> str:\n",
    "        x = x.lower()\n",
    "        x = title_re.sub(\"\", x)\n",
    "        x = remove_chars.sub(\"\", x)\n",
    "        # x = add_unk.sub(\"<unk>\", x)\n",
    "        x = add_unk.sub(\" \", x) # Remove ? from training data -> ? to <unk>\n",
    "        x = space_norm.sub(\" \", x)\n",
    "        return x.strip()\n",
    "\n",
    "    def clean_etruscan(x: str) -> str:\n",
    "        x = x.lower()\n",
    "        x = remove_chars.sub(\"\", x)\n",
    "        x = space_norm.sub(\" \", x)\n",
    "        return x.strip()\n",
    "    \n",
    "    et, eng = Data.load_translation_dataset(etruscan_fn=clean_etruscan, english_fn=clean_english)\n",
    "    tmp = pd.read_csv(etruscan_csv, index_col=0).dropna(subset=[\"Translation\"]).reset_index()\n",
    "    is_etp = tmp[\"key\"].isna().to_list()\n",
    "    \n",
    "    # Tokenizers\n",
    "    etruscan_tokenizer = Data.SentencePieceTokenizer(Data.ETRUSCAN)\n",
    "    english_tokenizer = Data.SentencePieceTokenizer(Data.ENGLISH)\n",
    "    etruscan_tokenizer.load(etruscan_model)\n",
    "    english_tokenizer.load(english_model)\n",
    "    \n",
    "    # Tokenization\n",
    "    et_encoded_char, et_encoded_word = etruscan_tokenizer.tokenize(et)\n",
    "    eng_encoded_char, eng_encoded_word = english_tokenizer.tokenize(eng)\n",
    "\n",
    "    # Dataset & Split\n",
    "    ds = datasets.Dataset.from_dict(\n",
    "        {     \n",
    "        \"inputs\": et,\n",
    "        \"inputs_chars\": et_encoded_char,\n",
    "        \"inputs_words\": et_encoded_word,\n",
    "        \"targets\": eng,\n",
    "        \"targets_chars\": eng_encoded_char,\n",
    "        \"targets_words\": eng_encoded_word,\n",
    "        \"is_etp\": is_etp\n",
    "        },\n",
    "    )\n",
    "    split = ds.train_test_split(train_size=train_size)\n",
    "\n",
    "    train = split[\"train\"]\n",
    "    test = split[\"test\"]\n",
    "\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, cached=cached)\n",
    "    test_dl = DataLoader(test, batch_size=batch_size, cached=cached)\n",
    "    return train_dl, test_dl, etruscan_tokenizer, english_tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl, etruscan_tokenizer, english_tokenizer = get_training_data(Data._dir + \"Etruscan.csv\", Data._dir + \"etruscan\", Data._dir + \"english\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 21)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"inputs_chars\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etruscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
